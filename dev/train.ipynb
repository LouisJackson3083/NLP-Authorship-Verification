{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2c753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import itertools\n",
    "import importlib\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad59321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '../src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "import prepare_data as ld\n",
    "import indexer\n",
    "import model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507c1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to reload our modules\n",
    "importlib.reload(indexer)\n",
    "importlib.reload(ld)\n",
    "importlib.reload(model_data)\n",
    "from indexer import Indexer\n",
    "from model_data import ModelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bfb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c8228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 30000 data points.\n",
      "Prepared 6000 data points.\n",
      "Training data size:  27000 \n",
      "Dev data size:  6000 \n",
      "Test data size:  3000\n"
     ]
    }
   ],
   "source": [
    "## Preparing data\n",
    "\n",
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS  = ld.prepare_data(\"../data/train.csv\", True)\n",
    "DEV_FIRST_TEXTS, DEV_SECOND_TEXTS, DEV_LABELS = ld.prepare_data(\"../data/dev.csv\", True)\n",
    "\n",
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS, \\\n",
    "TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS = ld.split_data(TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS)\n",
    "\n",
    "print(\"Training data size: \", len(TRAIN_FIRST_TEXTS),\n",
    "     \"\\nDev data size: \", len(DEV_FIRST_TEXTS), \n",
    "     \"\\nTest data size: \", len(TEST_FIRST_TEXTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7fa34c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting POS:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting POS:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Punctuation:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Punctuation:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Information:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Information:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up all data!\n"
     ]
    }
   ],
   "source": [
    "test = ld.PrepDataset(TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS)\n",
    "test.ExtractFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5eee8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1492, 1394, 1170, 728, 82]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.INVALID_INDEXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17b1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = Indexer(values=test.LABELS)\n",
    "POS_VALS = list(itertools.chain(*test.FIRST_POS + test.SECOND_POS))\n",
    "pos_indexer = Indexer(values=POS_VALS, pre=indexer.POS_PRE)\n",
    "# pos_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a8a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.index_data(pos_indexer, label_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c51e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "testset = ModelDataset(test, tokenizer, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652d106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': tensor([    3,    23,   114,    12,  3011,    12,    48,  2324,   116,     3,\n",
       "            23,  1731,    12,   473, 18940,    42,  1385, 14718,    10,     3,\n",
       "            23,    43,     3, 17010,    12,    34,   220,   648,   469,     5,\n",
       "         26378,    71,  7938,  1171,     9,  7622,  1008,    31,    17,   320,\n",
       "            44,   140,  2181,   239,    19,    78,  1627,   275,  8247,     6,\n",
       "            34,    31,     7,   614,    12, 13418,   852,    11,   258,     6,\n",
       "            27,   129,    16, 24875,  1029,    66,     8, 10393,     6,    27,\n",
       "            31,    51,    78, 30119,    27,   183,   786,   150,  1052,   125,\n",
       "            79,   497,  4467,     7,    54,    31,    17,   830,   140,   323,\n",
       "            27,   183,   786,    16,   334,   712,   194,  2163,     6,     1,\n",
       "         17600,     7, 10022,  1220, 29035,    30,  1244, 13017,    19,   352,\n",
       "            66,    91,    28,     8, 27432,  8929, 16023,  2908,     5,    37,\n",
       "         23470,    19,  2164,     6,    28,   192, 15765,  3372, 15752,    30,\n",
       "           893,   596,  9539,    28,  2806, 12314,     7,     6,    11,     8,\n",
       "          1345, 16729,     7,   577,   117,     3, 13708,   447,   913,     7,\n",
       "            30,     8,  1481,   117,   534,   984,    28,     3,     9,     3,\n",
       "            31,  6644, 25027,    31,    11,     8,  1004,    12,  2467, 13205,\n",
       "            38,    39,  1305,  8929, 16023,  1848,     5,   571,  1627,    24,\n",
       "           128,   502,    43,     3,     9,  2354, 11043,    12,   320,  1039,\n",
       "            12,     5,   419, 14481,     7,   140,    13,  2042, 10265,     1]),\n",
       " 'punc': tensor([   3,   10,    3,    5,    3,   31,    3,    6,    3,   31,    3,    6,\n",
       "            3,    6,    3,   31,    3,   31,    3,    6,    3,   31,    3,   31,\n",
       "            3,    6,    3,   31,    3,   31,    3,   31,    1, 1768,    3,    5,\n",
       "            3,    6,    3,    6,    3,  117,    3,  117,    3,   31,    3,   31,\n",
       "            3,    5,    3,    5,    3,    6,    3,    5,    1,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'info': tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'ipos': tensor([39, 10, 45, 34, 45, 21,  8, 24,  8, 10, 45, 34, 28, 41, 44, 28, 32,  8,\n",
       "         10, 26, 45, 37,  7, 39,  8, 29,  2,  2,  2, 10, 44, 34, 31, 37, 21,  8,\n",
       "         35, 44, 28, 41, 44, 18, 37, 35, 28, 45, 34, 44, 41, 44, 18, 37, 10, 28,\n",
       "         31, 43, 21,  8, 18, 37, 10, 44, 28, 37, 10, 28, 21,  8, 40, 37, 10, 39,\n",
       "         22, 44, 34, 37, 11, 37, 10, 28, 31, 21, 28,  8,  2, 18, 39, 22, 44, 34,\n",
       "         37, 11, 44, 10, 44, 37, 34, 37, 11,  8,  1, 39,  2, 31,  2,  2, 35, 19,\n",
       "         21, 31, 31, 21, 28,  2,  2,  8, 29, 21,  8, 35, 28, 18, 31,  7, 39, 10,\n",
       "          8, 31, 21,  8, 30, 31, 28, 39, 18, 41, 21, 39, 10, 32, 28, 39, 31, 21,\n",
       "          8, 32, 28, 39, 31, 21, 28,  8, 23, 41, 21,  8, 45, 34, 26, 31, 17, 28,\n",
       "          2,  2,  8, 29, 24, 28, 31, 21, 39, 10, 21, 28,  8, 45, 34, 44, 45, 29,\n",
       "         34, 37, 31,  2,  2, 39, 18, 24, 37, 30, 44, 28, 45, 34, 31,  2, 31,  2,\n",
       "         29,  1]),\n",
       " 'targets': tensor([1])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:av] *",
   "language": "python",
   "name": "conda-env-av-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
