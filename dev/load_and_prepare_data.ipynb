{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature extraction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from typing import List\n",
    "import re\n",
    "import string\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Quick test to see how many duplicates are in these csv's\n",
    "df_1 = pd.read_csv(\"../data/dev.csv\")\n",
    "df_2 = pd.read_csv(\"../data/train.csv\")\n",
    "duplicate_count = 0\n",
    "for i in range(len(df_1)):\n",
    "    if (df_2[df_2.eq(df_1.iloc[i, 0]).any(axis=1)].empty == False):\n",
    "        df_2_index = int(df_2[df_2.eq(df_1.iloc[i, 0]).any(axis=1)].iloc[0].name)\n",
    "        if (df_2.iloc[df_2_index,1] == df_1.iloc[i, 1]):\n",
    "            duplicate_count += 1\n",
    "    \n",
    "    \n",
    "    if (df_2[df_2.eq(df_1.iloc[i, 1]).any(axis=1)].empty == False):\n",
    "        df_2_index = int(df_2[df_2.eq(df_1.iloc[i, 1]).any(axis=1)].iloc[0].name)\n",
    "        if (df_2.iloc[df_2_index,0] == df_1.iloc[i, 0]):\n",
    "            duplicate_count += 1\n",
    "\n",
    "print(duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    FIRST_TEXTS: List[str],\n",
    "    SECOND_TEXTS: List[str],\n",
    "    LABELS: List[int],\n",
    "    test_split: float = 0.1\n",
    "    ) -> [List[str], List[str], List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a set of first and second texts and their labels into a training/test split\n",
    "    args:\n",
    "        FIRST_TEXTS,\n",
    "        SECOND_TEXTS,\n",
    "        LABELS,\n",
    "        test_split: float - the amount of test data points to extract from the existing set of data\n",
    "    returns:\n",
    "        2 sets of data\n",
    "    \"\"\"\n",
    "\n",
    "    # Init test lists and amended lists\n",
    "    TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS = [],[],[]\n",
    "    TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS = [],[],[]\n",
    "\n",
    "    TRAIN_FIRST_TEXTS, TEST_FIRST_TEXTS, TRAIN_SECOND_TEXTS, \\\n",
    "    TEST_SECOND_TEXTS, TRAIN_LABELS, TEST_LABELS = train_test_split(\n",
    "        FIRST_TEXTS, \n",
    "        SECOND_TEXTS, \n",
    "        LABELS, \n",
    "        test_size=test_split,\n",
    "        random_state=1234\n",
    "    )\n",
    "\n",
    "    return TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS, TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(csv_filepath: str, verbose: bool = False) -> [List[str], List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Prepares the data from csv files\n",
    "    args:\n",
    "        csv_filepath: str - the filepath of the csv file to load our authorship verification data from\n",
    "        verbose: bool\n",
    "    returns:\n",
    "        FIRST_TEXTS: List[str] - list of strings in the first column of csv \n",
    "        SECOND_TEXTS: List[str] - list of strings in the second column of csv \n",
    "        LABELS: List[int] - one hot encoded labels for whether the first and second text are from the same author\n",
    "    \"\"\"\n",
    "    FIRST_TEXTS, SECOND_TEXTS, LABELS = [],[],[]\n",
    "\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    # Iterate through and add to our first and second texts and labels\n",
    "    # I was initially worried about this being slow but it manages to do this on the train dataset very fast\n",
    "    for i in range(int(len(df) / 100)):\n",
    "        FIRST_TEXTS.append(df.iloc[i, 0])\n",
    "        SECOND_TEXTS.append(df.iloc[i, 1])\n",
    "        LABELS.append(df.iloc[i, 2])\n",
    "    \n",
    "    # Ensure that the data is valid\n",
    "    assert len(FIRST_TEXTS) == len(SECOND_TEXTS) == len(LABELS)\n",
    "    if verbose: print(\"Prepared\", len(df), \"data points.\")\n",
    "\n",
    "    return FIRST_TEXTS, SECOND_TEXTS, LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 30000 data points.\n",
      "Prepared 6000 data points.\n",
      "Training data size:  270 \n",
      "Dev data size:  60 \n",
      "Test data size:  30\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS  = prepareData(\"../data/train.csv\", True)\n",
    "DEV_FIRST_TEXTS, DEV_SECOND_TEXTS, DEV_LABELS = prepareData(\"../data/dev.csv\", True)\n",
    "\n",
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS, \\\n",
    "TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS = split_data(TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS)\n",
    "\n",
    "print(\"Training data size: \", len(TRAIN_FIRST_TEXTS),\n",
    "     \"\\nDev data size: \", len(DEV_FIRST_TEXTS), \n",
    "     \"\\nTest data size: \", len(TEST_FIRST_TEXTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \"\"\"\n",
    "    For a given dataset, whether that's the training, dev, or \n",
    "    test dataset, this class handles most of it's setup and functions.\n",
    "\n",
    "    We will do all tokenization, feature extraction within this class and it will be what we pass to the model etc.\n",
    "\n",
    "    Also, some texts are invalid or aren't actually pairs, we collate a list of indexes to pop from first_texts once all\n",
    "    the pos, punctuation and info is calculated\n",
    "\n",
    "    Variables:\n",
    "        FIRST_TEXTS - a list of the first texts,\n",
    "        SECOND_TEXTS - a list of the second texts,\n",
    "        LABELS - the binary classification (0 or 1) of whether this is from the same author,\n",
    "        FIRST_TOKENS - a list of the first texts tokenized,\n",
    "        SECOND_TOKENS - a list of the first texts tokenized,\n",
    "        FIRST_POS - a list of the first texts with the POS tagged,\n",
    "        SECOND_POS - a list of the second texts with the POS tagged,\n",
    "        FIRST_PUNCTUATION - a list of the first texts' punctuation,\n",
    "        SECOND_PUNCTUATION - a list of the second texts' punctuation,\n",
    "        FIRST_INFO - a list of the first texts' auxillary info,\n",
    "        SECOND_INFO - a list of the second texts' auxillary info,\n",
    "        INVALID_INDEXES - a set of indexes that are invalid and should be popped at the end of feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, FIRST_TEXTS, SECOND_TEXTS, LABELS):\n",
    "        self.FIRST_TEXTS = FIRST_TEXTS\n",
    "        self.SECOND_TEXTS = SECOND_TEXTS\n",
    "        self.LABELS = LABELS\n",
    "        self.INVALID_INDEXES = []\n",
    "    \n",
    "    def ExtractFeatures(self):\n",
    "        # Extract POS from the texts\n",
    "        \n",
    "        #wordtokenizer = word_tokenize\n",
    "        #pos_tagger=pos_tag\n",
    "        \n",
    "        print(\"(1/3) Extracting POS . . .\")\n",
    "        self.FIRST_POS = self.ExtractPOS(self.FIRST_TEXTS)\n",
    "        self.SECOND_POS = self.ExtractPOS(self.SECOND_TEXTS)\n",
    "\n",
    "        print(\"(2/3) Extracting Punctuation . . .\")\n",
    "        self.FIRST_PUNCTUATION = self.ExtractPunctuation(self.FIRST_TEXTS)\n",
    "        self.SECOND_PUNCTUATION = self.ExtractPunctuation(self.SECOND_TEXTS)\n",
    "\n",
    "        print(\"(3/3) Extracting Information . . .\")\n",
    "        self.FIRST_INFORMATION = self.ExtractInformation(self.FIRST_TEXTS)\n",
    "        self.SECOND_INFORMATION = self.ExtractInformation(self.SECOND_TEXTS)\n",
    "\n",
    "        self.CleanUpData()\n",
    "        \n",
    "\n",
    "    def ExtractPOS(self, TEXTS):\n",
    "        POS = []\n",
    "        \n",
    "        # Get sets of all punctuation and all emojis\n",
    "        all_punctuation = set(string.punctuation)\n",
    "        all_emojis = set(emoji.EMOJI_DATA)\n",
    "        # We then append both punctuation and emojis to our symbols variable,\n",
    "        # and remove the < > tag markers\n",
    "        valid_symbols = all_punctuation | all_emojis\n",
    "        valid_symbols.remove(\">\")\n",
    "        valid_symbols.remove(\"<\")\n",
    "\n",
    "        # For each text in the list\n",
    "        for index, text in tqdm(enumerate(TEXTS), total=len(TEXTS),\n",
    "                                leave=False, desc=\"Extracting POS\"):\n",
    "            # For each word, get it's part of speech\n",
    "            try:\n",
    "                tags = pos_tag(word_tokenize(text))\n",
    "                POS.append([\n",
    "                    token for _word, token in tags if (token not in valid_symbols)\n",
    "                ])\n",
    "                assert POS[-1] is not []\n",
    "            except Exception:\n",
    "                self.INVALID_INDEXES.append(index)\n",
    "                POS.append([])\n",
    "\n",
    "        return POS\n",
    "    \n",
    "    def ExtractPunctuation(self, TEXTS):\n",
    "        PUNCTUATION = []\n",
    "        # Get sets of all punctuation and all emojis\n",
    "        all_punctuation = set(string.punctuation)\n",
    "        all_emojis = set(emoji.EMOJI_DATA)\n",
    "        # We then append both punctuation and emojis to our symbols variable,\n",
    "        # and remove the < > tag markers\n",
    "        valid_symbols = all_punctuation | all_emojis\n",
    "        valid_symbols.remove(\">\")\n",
    "        valid_symbols.remove(\"<\")\n",
    "\n",
    "        pattern = r\"(?<=\\<).*?(?=\\>)\"\n",
    "        for index, text in enumerate(TEXTS):\n",
    "            try:\n",
    "                text = re.sub(pattern, \"\", text)\n",
    "                punc = \" \".join(ch for ch in text if ch in valid_symbols)\n",
    "                \n",
    "                PUNCTUATION.append(punc)\n",
    "            except: # If the text returns invalid, add it to invalid indexes\n",
    "                self.INVALID_INDEXES.append(index)\n",
    "                PUNCTUATION.append([])\n",
    "            \n",
    "        \n",
    "        return PUNCTUATION\n",
    "    \n",
    "    def ExtractInformation(self, TEXTS):\n",
    "        INFORMATION = []\n",
    "        pattern = r\"(?<=\\<).*?(?=\\>)\"\n",
    "        \n",
    "        # Iterate through every text\n",
    "        for index, text in enumerate(TEXTS):\n",
    "            try:\n",
    "                text = re.findall(pattern, text)\n",
    "                INFORMATION.append(\" \".join(text))\n",
    "            except: # If the text returns invalid, add it to invalid indexes\n",
    "                self.INVALID_INDEXES.append(index)\n",
    "                INFORMATION.append([])\n",
    "\n",
    "        return INFORMATION\n",
    "    \n",
    "    def CleanUpData(self):\n",
    "        try: # Before we clean up, let's make sure we actually have all our lists.\n",
    "            self.FIRST_POS\n",
    "            self.SECOND_POS\n",
    "            self.FIRST_PUNCTUATION\n",
    "            self.SECOND_PUNCTUATION\n",
    "            self.FIRST_INFORMATION\n",
    "            self.SECOND_INFORMATION\n",
    "        except:\n",
    "            print(\"Not every feature has been extracted. Please check your code and try again.\")\n",
    "            return\n",
    "\n",
    "        # Convert list into a set to avoid duplicates\n",
    "        self.INVALID_INDEXES = set(self.INVALID_INDEXES)\n",
    "        self.INVALID_INDEXES = sorted(self.INVALID_INDEXES, reverse=True)\n",
    "\n",
    "        for index in self.INVALID_INDEXES:\n",
    "            self.FIRST_TEXTS.pop(index)\n",
    "            self.SECOND_TEXTS.pop(index)\n",
    "            self.FIRST_POS.pop(index)\n",
    "            self.SECOND_POS.pop(index)\n",
    "            self.FIRST_PUNCTUATION.pop(index)\n",
    "            self.SECOND_PUNCTUATION.pop(index)\n",
    "            self.FIRST_INFORMATION.pop(index)\n",
    "            self.SECOND_INFORMATION.pop(index)\n",
    "\n",
    "        print(\"Cleaned up all data!\")\n",
    "\n",
    "    def IndexFeatures(self):\n",
    "        pass\n",
    "        #TODO: create indexer & tokenindexer class\n",
    "        #TODO: index the labels, and POS using a indexer and tokenindexer class\n",
    "        # convert our labels into what they're doing?!?!\n",
    "\n",
    "\n",
    "        #TODO: make function that returns a dict in the format:\n",
    "        # def DATACOLUMNS:\n",
    "        #   data_dict =   {\"first_text\": FIRST_TEXT,\n",
    "                        #  \"second_text\": SECOND_TEXT\n",
    "                        #  \"first_punctuations\": FIRST_PUNCTUATIONS,\n",
    "                        #  \"second_punctuations\": SECONDS_PUNCTUATIONS,\n",
    "                        #  \"first_information\": FIRST_INFORMATION,\n",
    "                        #  \"second_information\": SECOND_INFORMATION,\n",
    "                        #  \"first_pos\": FIRST_POS_INDEXED,\n",
    "                        #  \"second_pos\": SECOND_POS_INDEXED,\n",
    "                        #  \"targets\": LABELS_INDEXED}\n",
    "\n",
    "        #LABEL_INDEXER = Indexer(vocabs=self.LABEL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Extracting POS . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2/3) Extracting Punctuation . . .\n",
      "(3/3) Extracting Information . . .\n",
      "Cleaned up all data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test = Dataset(TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS)\n",
    "test.ExtractFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Extracting POS . . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2/3) Extracting Punctuation . . .\n",
      "(3/3) Extracting Information . . .\n",
      "Cleaned up all data!\n",
      "['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'NNPS', 'TO', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'JJ', 'NN', 'NNP', 'NNP', 'NNP', 'WP', 'DT', 'NN', 'CC', 'JJ', 'NN', 'PRP', 'RB', 'VBP', 'NNP', 'NNP', 'PRP', 'VBP', 'RB', 'VBN', 'IN', 'IN', 'PRP', 'PRP', 'RB', 'VBD', 'TO', 'DT', 'NN', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'PRP', 'MD', 'RB', 'VB', 'WRB', 'JJ', 'PRP', 'VBP', 'IN', 'DT', 'NNPS', 'CC', 'NNS', 'IN', 'DT', 'NNP', 'CC', 'MD', 'VB', 'VBG', 'DT', 'NN', 'TO', 'VB', 'NN', 'IN', 'PRP', 'RB', 'NNP', 'PRP', 'IN', 'VBG', 'DT', 'JJ', 'NN', 'PRP', 'VBP', 'NNP', 'NNP', 'PRP', 'VBP', 'IN', 'PRP', 'VBP', 'WP', 'PRP', 'VBP', 'CC', 'PRP', 'VBD', 'VBN', 'IN', 'DT', 'NN', 'IN', 'CD', 'NNS', 'VBG', 'VBN', 'VBN', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'CD', 'WRB', 'PRP', 'RB', 'VBD', 'TO', 'NNP']\n",
      "['DT', 'NN', 'VBZ', 'RB', 'RB', 'VBN', 'VBN', 'RB', 'IN', 'PRP$', 'NN', 'IN', 'DT', 'NNS', 'CC', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'NN', 'IN', 'DT', 'RB', 'IN', 'PRP$', 'JJ', 'NN', 'PRP', 'VBP', 'RB', 'VBN', 'TO', 'VB', 'PRP', 'IN', 'DT', 'PRP', 'VBZ', 'VB', 'NNP', 'NNP', 'VBG', 'DT', 'NNP', 'IN', 'RB', 'RB', 'IN', 'IN', 'DT', 'JJ', 'NN', 'PRP', 'VBZ', 'VBN', 'NNP', 'RB', 'IN', 'PRP$', 'JJ', 'NN', 'VBZ', 'VBN', 'NNP', 'DT', 'NN', 'POS', 'NN', 'NN', 'IN', 'WDT', 'DT', 'JJ', 'NN', 'VBZ', 'VBG', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'VBZ', 'VBG', 'RB', 'CC', 'IN', 'PRP', 'VBZ', 'RP', 'PRP', 'VBZ', 'RB', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', 'NN', 'WDT', 'MD', 'TO', 'VB', 'PRP', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'RB', 'RB', 'IN', 'DT', 'JJ', 'NN', 'CD', 'VBZ', 'VBN', 'JJ', 'IN', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'PRP', 'RB', 'RB', 'VBZ', 'RB', 'IN', 'CD', 'NN', 'TO', 'DT', 'IN']\n",
      "['PRP', 'MD', 'VB', 'IN', 'NN', 'JJ', 'JJ', 'CC', 'VB', 'IN', 'PRP', 'VBP', 'PRP']\n",
      "['NNP', 'POS', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NNP', 'NNP', 'VBZ', 'RB', 'DT', 'JJ', 'NN', 'PRP', 'RB', 'VBP', 'NN', 'JJ', 'RB', 'EX', 'MD', 'VB', 'DT', 'NN', 'CC', 'RB', 'DT', 'NN', 'NN']\n",
      "['VBN', 'IN', 'NNP', 'NNP', 'IN', 'CD', 'CD', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'VB', 'JJ', 'IN', 'PRP$', 'JJ', 'VBG', 'DT', 'VBN', 'RB', 'VBN', 'NN', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'NNP', 'VB', 'IN', 'PRP', 'MD', 'VB', 'IN', 'JJ', 'NN']\n",
      "['VBN', 'IN', 'NNP', 'NNP', 'NNP', 'IN', 'CD', 'CD', 'NNP', 'NN', 'NN', 'NNP', 'NN', 'NNP', 'NNP', 'NNP', 'IN', 'CD', 'CD', 'NNPS', 'TO', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'JJ', 'NN', 'NN', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'NNP', 'NN', 'JJ', 'NN', 'NN', 'NNP', 'CD', 'NNP', 'CD', 'CD', 'NN', 'NNP', 'IN', 'NN', 'NN', 'NNP', 'NN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'NNP', 'VBD', 'NNP', 'NN', 'NN', 'NNP', 'NN', 'NN', 'NNP', 'NNP', 'CD', 'IN', 'NNP', 'VBZ', 'JJ', 'IN', 'PRP', 'RB', 'PRP', 'MD', 'VB', 'PRP', 'RB', 'IN', 'PRP$', 'NN', 'IN', 'NN', 'NNP', 'IN', 'NNP', 'CD', 'NNP', 'CD', 'NNP', 'NNP', 'VBD', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'PRP', 'MD', 'VB', 'JJ', 'TO', 'VB', 'IN', 'PRP', 'VB', 'WRB', 'RB', 'CD', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'NN', 'VBD', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'PRP', 'VBD', 'VBG', 'TO', 'DT', 'NNS', 'PRP', 'VBP', 'IN', 'NNP', 'IN', 'VBG', 'JJ', 'NN', 'NNS', 'IN', 'JJ', 'NN', 'CC', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'NN', 'NNS', 'IN', 'PRP', 'VBP', 'JJ', 'IN', 'VBG', 'TO', 'PRP', 'IN', 'NNS', 'NNS']\n",
      "['VBN', 'IN', 'NNP', 'NNP', 'IN', 'CD', 'CD', 'NNP', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'IN', 'CD', 'CD', 'NNPS', 'TO', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NN', 'JJ', 'NNP', 'NNP', 'NNP', 'NNP', 'VB', 'IN', 'PRP$', 'NN', 'IN', 'NN', 'IN', 'NNP', 'NNP', 'CD', 'IN', 'DT', 'NN', 'IN', 'NN', 'CC', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'NNP', 'NN', 'IN', 'CD', 'CC', 'CD', 'NN', 'TO', 'VB', 'PRP', 'DT', 'PRP$', 'NN', 'VBZ', 'CD', 'NNP', 'NNP', 'NNP', 'VB', 'NNP', 'NNP', 'TO', 'NNP', 'CC', 'RB', 'VB', 'RB', 'IN', 'NNP', 'IN', 'NNP', 'NNP', 'CC', 'NNP', 'PRP$', 'NN', 'MD', 'VB', 'IN', 'PRP$', 'NN', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'NN']\n",
      "['VB', 'RP', 'DT', 'NN', 'JJ', 'NN', 'NN', 'NN']\n",
      "['RB', 'IN', 'PRP', 'VBD', 'RP', 'IN', 'DT', 'NNS', 'NNP', 'NNP', 'VBG', 'JJ', 'NNP', 'NN', 'IN', 'NNP', 'NN', 'NN', 'CC', 'NNP', 'NNP', 'WP', 'NN', 'IN', 'CD', 'TO', 'CD', 'IN', 'NN', 'IN', 'EX', 'RB', 'JJ', '``', 'NN', '``', 'VBZ', 'DT', 'NNS', 'VBP', 'RB', 'JJ', 'EX', 'VBZ', 'RB', 'CD', 'JJ', 'CD', 'CC', 'DT', 'NN', 'CC', 'NN', 'VBP', 'RB', 'JJ', 'RB', 'PRP', 'VBZ', 'DT', 'NN', 'VBN', 'NNP', 'JJ', 'NN', 'WP', 'VBZ', 'CD', 'IN', 'DT', 'JJS', 'NNS', 'IN', 'NN', 'NN', 'PRP$', 'NN', 'IN', 'VBG', 'VBZ', 'NNP', 'DT', 'NN', 'JJ', 'NN', 'RB', 'PRP', 'NN', 'IN', 'VBD', 'PRP', 'PRP', 'VBD', 'DT', 'CD', 'NN', 'IN', 'IN', 'DT', 'NN', 'IN', 'DT', 'CD', 'PRP', 'MD', 'VB', 'TO', 'DT', 'DT', 'NNS', 'DT', 'NN', 'DT', 'NNS', 'DT', 'JJ', 'NN', 'CC', 'NNS', 'DT', 'JJ', 'IN', 'PRP', 'PRP', 'VBD', 'DT', 'NN', 'RB', 'TO', 'DT', 'NN', 'RB', 'DT', 'NN', 'VBZ', 'PRP$', 'DT', 'TO', 'DT', 'NN']\n",
      "['RBR', 'NNP', 'NNP', 'NN', 'PRP$', 'NN', 'NN', 'CC', 'PRP', 'VBD', 'VBG', 'IN', 'DT', 'JJ', 'NN', 'NN', 'VBN', 'NNP', 'JJ', 'NN', 'PRP', 'VBD', 'VBN', 'RP', 'IN', 'DT', 'NNP', 'NNP', 'NN', 'IN', 'DT', 'NNS', 'NN', 'IN', 'NNP', 'POS', 'NN', 'PRP', 'VBD', 'TO', 'VB', 'IN', 'DT', 'JJ', 'NN', 'CC', 'PRP', 'VBD', 'NNP', 'NNP', 'NN', 'TO', 'VB', 'PRP$', 'JJ', 'NN', 'VBD', 'RB', 'VB', 'IN', 'DT', 'RB', 'WP', 'VBZ', 'DT', 'VB', 'TO', 'VB', 'IN', 'NN', 'IN', 'PRP', 'VBP', 'VBG', 'JJ', 'TO', 'VB', 'PRP', 'VBD', 'VBG', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NN', 'CC', 'WP', 'PRP', 'VBZ', 'IN', 'DT', 'NN', 'NNP', 'PRP', 'VBD', 'RB', 'VBN', 'DT', 'NN', 'NNS', 'PRP', 'VBD', 'IN', 'PRP', 'CC', 'PRP', 'VBD', 'PRP', 'CC', 'PRP$', 'NN', 'DT', 'NN', 'PRP', 'RB', 'VBD', 'CD', 'IN', 'PRP$', 'NN', 'POS', 'NNS', 'IN', 'PRP$', 'NN', 'PRP', 'VBP', 'VBN', 'VBG', 'TO', 'VB', 'IN', 'NN']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dev = Dataset(DEV_FIRST_TEXTS, DEV_SECOND_TEXTS, DEV_LABELS)\n",
    "dev.ExtractFeatures()\n",
    "\n",
    "for i in range(10):\n",
    "    print(dev.FIRST_POS[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Extracting POS . . .\n",
      "(2/3) Extracting Punctuation . . .\n",
      "(3/3) Extracting Information . . .\n",
      "Cleaned up all data!\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS)\n",
    "train.ExtractFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   37, 32099, 10681,    16, 32098,  2447,     1]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "## USE THIS: LightningDataModule\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "print(input_ids)\n",
    "print(type(input_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
