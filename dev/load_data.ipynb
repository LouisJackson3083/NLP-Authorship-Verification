{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature extraction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from typing import List\n",
    "import re\n",
    "import string\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Quick test to see how many duplicates are in these csv's\n",
    "df_1 = pd.read_csv(\"../data/dev.csv\")\n",
    "df_2 = pd.read_csv(\"../data/train.csv\")\n",
    "duplicate_count = 0\n",
    "for i in range(len(df_1)):\n",
    "    if (df_2[df_2.eq(df_1.iloc[i, 0]).any(axis=1)].empty == False):\n",
    "        df_2_index = int(df_2[df_2.eq(df_1.iloc[i, 0]).any(axis=1)].iloc[0].name)\n",
    "        if (df_2.iloc[df_2_index,1] == df_1.iloc[i, 1]):\n",
    "            duplicate_count += 1\n",
    "    \n",
    "    \n",
    "    if (df_2[df_2.eq(df_1.iloc[i, 1]).any(axis=1)].empty == False):\n",
    "        df_2_index = int(df_2[df_2.eq(df_1.iloc[i, 1]).any(axis=1)].iloc[0].name)\n",
    "        if (df_2.iloc[df_2_index,0] == df_1.iloc[i, 0]):\n",
    "            duplicate_count += 1\n",
    "\n",
    "print(duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    FIRST_TEXTS: List[str],\n",
    "    SECOND_TEXTS: List[str],\n",
    "    LABELS: List[int],\n",
    "    test_split: float = 0.1\n",
    "    ) -> [List[str], List[str], List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a set of first and second texts and their labels into a training/test split\n",
    "    args:\n",
    "        FIRST_TEXTS,\n",
    "        SECOND_TEXTS,\n",
    "        LABELS,\n",
    "        test_split: float - the amount of test data points to extract from the existing set of data\n",
    "    returns:\n",
    "        2 sets of data\n",
    "    \"\"\"\n",
    "\n",
    "    # Init test lists and amended lists\n",
    "    TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS = [],[],[]\n",
    "    TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS = [],[],[]\n",
    "\n",
    "    TRAIN_FIRST_TEXTS, TEST_FIRST_TEXTS, TRAIN_SECOND_TEXTS, \\\n",
    "    TEST_SECOND_TEXTS, TRAIN_LABELS, TEST_LABELS = train_test_split(\n",
    "        FIRST_TEXTS, \n",
    "        SECOND_TEXTS, \n",
    "        LABELS, \n",
    "        test_size=test_split,\n",
    "        random_state=1234\n",
    "    )\n",
    "\n",
    "    return TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS, TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(csv_filepath: str, verbose: bool = False) -> [List[str], List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Prepares the data from csv files\n",
    "    args:\n",
    "        csv_filepath: str - the filepath of the csv file to load our authorship verification data from\n",
    "        verbose: bool\n",
    "    returns:\n",
    "        FIRST_TEXTS: List[str] - list of strings in the first column of csv \n",
    "        SECOND_TEXTS: List[str] - list of strings in the second column of csv \n",
    "        LABELS: List[int] - one hot encoded labels for whether the first and second text are from the same author\n",
    "    \"\"\"\n",
    "    FIRST_TEXTS, SECOND_TEXTS, LABELS = [],[],[]\n",
    "\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    # Iterate through and add to our first and second texts and labels\n",
    "    # I was initially worried about this being slow but it manages to do this on the train dataset very fast\n",
    "    for i in range(len(df)):\n",
    "        FIRST_TEXTS.append(df.iloc[i, 0])\n",
    "        SECOND_TEXTS.append(df.iloc[i, 1])\n",
    "        LABELS.append(df.iloc[i, 2])\n",
    "    \n",
    "    # Ensure that the data is valid\n",
    "    assert len(FIRST_TEXTS) == len(SECOND_TEXTS) == len(LABELS)\n",
    "    if verbose: print(\"Prepared\", len(df), \"data points.\")\n",
    "\n",
    "    return FIRST_TEXTS, SECOND_TEXTS, LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 30000 data points.\n",
      "Prepared 6000 data points.\n",
      "Training data size:  27000 \n",
      "Dev data size:  6000 \n",
      "Test data size:  3000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS  = prepareData(\"../data/train.csv\", True)\n",
    "DEV_FIRST_TEXTS, DEV_SECOND_TEXTS, DEV_LABELS = prepareData(\"../data/dev.csv\", True)\n",
    "\n",
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS, \\\n",
    "TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS = split_data(TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS)\n",
    "\n",
    "print(\"Training data size: \", len(TRAIN_FIRST_TEXTS),\n",
    "     \"\\nDev data size: \", len(DEV_FIRST_TEXTS), \n",
    "     \"\\nTest data size: \", len(TEST_FIRST_TEXTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \"\"\"\n",
    "    For a given dataset, whether that's the training, dev, or \n",
    "    test dataset, this class handles most of it's setup and functions.\n",
    "\n",
    "    We will do all tokenization, feature extraction within this class and it will be what we pass to the model etc.\n",
    "\n",
    "    Also, some texts are invalid or aren't actually pairs, we collate a list of indexes to pop from first_texts once all\n",
    "    the pos, punctuation and info is calculated\n",
    "\n",
    "    Variables:\n",
    "        FIRST_TEXTS - a list of the first texts,\n",
    "        SECOND_TEXTS - a list of the second texts,\n",
    "        LABELS - the binary classification (0 or 1) of whether this is from the same author,\n",
    "        FIRST_TOKENS - a list of the first texts tokenized,\n",
    "        SECOND_TOKENS - a list of the first texts tokenized,\n",
    "        FIRST_POS - a list of the first texts with the POS tagged,\n",
    "        SECOND_POS - a list of the second texts with the POS tagged,\n",
    "        FIRST_PUNCTUATION - a list of the first texts' punctuation,\n",
    "        SECOND_PUNCTUATION - a list of the second texts' punctuation,\n",
    "        FIRST_INFO - a list of the first texts' auxillary info,\n",
    "        SECOND_INFO - a list of the second texts' auxillary info,\n",
    "        INVALID_INDEXES - a set of indexes that are invalid and should be popped at the end of feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, FIRST_TEXTS, SECOND_TEXTS, LABELS):\n",
    "        self.FIRST_TEXTS = FIRST_TEXTS\n",
    "        self.SECOND_TEXTS = SECOND_TEXTS\n",
    "        self.LABELS = LABELS\n",
    "        self.INVALID_INDEXES = []\n",
    "    \n",
    "    def ExtractFeatures(self):\n",
    "        # Extract POS from the texts\n",
    "        \n",
    "        #wordtokenizer = word_tokenize\n",
    "        #pos_tagger=pos_tag\n",
    "        \n",
    "        print(\"(1/3) Extracting POS . . .\")\n",
    "        self.FIRST_POS = self.ExtractPOS(self.FIRST_TEXTS)\n",
    "        self.SECOND_POS = self.ExtractPOS(self.SECOND_TEXTS)\n",
    "\n",
    "        print(\"(2/3) Extracting Punctuation . . .\")\n",
    "        self.FIRST_PUNCTUATION = self.ExtractPunctuation(self.FIRST_TEXTS)\n",
    "        self.SECOND_PUNCTUATION = self.ExtractPunctuation(self.SECOND_TEXTS)\n",
    "\n",
    "        print(\"(3/3) Extracting Information . . .\")\n",
    "        self.FIRST_INFORMATION = self.ExtractInformation(self.FIRST_TEXTS)\n",
    "        self.SECOND_INFORMATION = self.ExtractInformation(self.SECOND_TEXTS)\n",
    "\n",
    "        self.CleanUpData()\n",
    "        \n",
    "\n",
    "    def ExtractPOS(self, TEXTS):\n",
    "        POS = []\n",
    "        # For each text in the list\n",
    "        for index, text in enumerate(TEXTS):\n",
    "            curr_POS = []\n",
    "            # For each word, get it's part of speech\n",
    "            try:\n",
    "                tags = pos_tag(word_tokenize(text))\n",
    "                for word in tags:\n",
    "                    curr_POS.append(word)\n",
    "            except: # If the text returns invalid, add it to invalid indexes\n",
    "                self.INVALID_INDEXES.append(index)\n",
    "                POS.append([])\n",
    "            \n",
    "            POS.append(curr_POS)\n",
    "        return POS\n",
    "    \n",
    "    def ExtractPunctuation(self, TEXTS):\n",
    "        PUNCTUATION = []\n",
    "        # Get sets of all punctuation and all emojis\n",
    "        all_punctuation = set(string.punctuation)\n",
    "        all_emojis = set(emoji.EMOJI_DATA)\n",
    "        # We then append both punctuation and emojis to our symbols variable,\n",
    "        # and remove the < > tag markers\n",
    "        valid_symbols = all_punctuation | all_emojis\n",
    "        valid_symbols.remove(\">\")\n",
    "        valid_symbols.remove(\"<\")\n",
    "\n",
    "        pattern = r\"(?<=\\<).*?(?=\\>)\"\n",
    "        for index, text in enumerate(TEXTS):\n",
    "            try:\n",
    "                text = re.sub(pattern, \"\", text)\n",
    "                punc = \" \".join(ch for ch in text if ch in valid_symbols)\n",
    "                PUNCTUATION.append(punc)\n",
    "            except: # If the text returns invalid, add it to invalid indexes\n",
    "                self.INVALID_INDEXES.append(index)\n",
    "                PUNCTUATION.append([])\n",
    "        \n",
    "        return PUNCTUATION\n",
    "    \n",
    "    def ExtractInformation(self, TEXTS):\n",
    "        INFORMATION = []\n",
    "        pattern = r\"(?<=\\<).*?(?=\\>)\"\n",
    "        \n",
    "        # Iterate through every text\n",
    "        for index, text in enumerate(TEXTS):\n",
    "            try:\n",
    "                text = re.findall(pattern, text)\n",
    "                INFORMATION.append(\" \".join(text))\n",
    "            except: # If the text returns invalid, add it to invalid indexes\n",
    "                self.INVALID_INDEXES.append(index)\n",
    "                INFORMATION.append([])\n",
    "\n",
    "        return INFORMATION\n",
    "    \n",
    "    def CleanUpData(self):\n",
    "        try: # Before we clean up, let's make sure we actually have all our lists.\n",
    "            self.FIRST_POS\n",
    "            self.SECOND_POS\n",
    "            self.FIRST_PUNCTUATION\n",
    "            self.SECOND_PUNCTUATION\n",
    "            self.FIRST_INFORMATION\n",
    "            self.SECOND_INFORMATION\n",
    "        except:\n",
    "            print(\"Not every feature has been extracted. Please check your code and try again.\")\n",
    "            return\n",
    "\n",
    "        # Convert list into a set to avoid duplicates\n",
    "        self.INVALID_INDEXES = set(self.INVALID_INDEXES)\n",
    "        self.INVALID_INDEXES = sorted(self.INVALID_INDEXES, reverse=True)\n",
    "\n",
    "        for index in self.INVALID_INDEXES:\n",
    "            self.FIRST_TEXTS.pop(index)\n",
    "            self.SECOND_TEXTS.pop(index)\n",
    "            self.FIRST_POS.pop(index)\n",
    "            self.SECOND_POS.pop(index)\n",
    "            self.FIRST_PUNCTUATION.pop(index)\n",
    "            self.SECOND_PUNCTUATION.pop(index)\n",
    "            self.FIRST_INFORMATION.pop(index)\n",
    "            self.SECOND_INFORMATION.pop(index)\n",
    "\n",
    "        print(\"Cleaned up all data!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Extracting POS . . .\n",
      "(2/3) Extracting Punctuation . . .\n",
      "(3/3) Extracting Information . . .\n",
      "Cleaned up all data!\n"
     ]
    }
   ],
   "source": [
    "test = Dataset(TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS)\n",
    "test.ExtractFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Extracting POS . . .\n",
      "(2/3) Extracting Punctuation . . .\n",
      "(3/3) Extracting Information . . .\n",
      "Cleaned up all data!\n"
     ]
    }
   ],
   "source": [
    "dev = Dataset(DEV_FIRST_TEXTS, DEV_SECOND_TEXTS, DEV_LABELS)\n",
    "dev.ExtractFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) Extracting POS . . .\n",
      "(2/3) Extracting Punctuation . . .\n",
      "(3/3) Extracting Information . . .\n",
      "Cleaned up all data!\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS)\n",
    "train.ExtractFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   37, 32099, 10681,    16, 32098,  2447,     1]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "## USE THIS: LightningDataModule\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "print(input_ids)\n",
    "print(type(input_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
