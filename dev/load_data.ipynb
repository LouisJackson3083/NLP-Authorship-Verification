{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Quick test to see how many duplicates are in these csv's\n",
    "df_1 = pd.read_csv(\"../data/dev.csv\")\n",
    "df_2 = pd.read_csv(\"../data/train.csv\")\n",
    "duplicate_count = 0\n",
    "for i in range(len(df_1)):\n",
    "    if (df_2[df_2.eq(df_1.iloc[i, 0]).any(axis=1)].empty == False):\n",
    "        df_2_index = int(df_2[df_2.eq(df_1.iloc[i, 0]).any(axis=1)].iloc[0].name)\n",
    "        if (df_2.iloc[df_2_index,1] == df_1.iloc[i, 1]):\n",
    "            duplicate_count += 1\n",
    "    \n",
    "    \n",
    "    if (df_2[df_2.eq(df_1.iloc[i, 1]).any(axis=1)].empty == False):\n",
    "        df_2_index = int(df_2[df_2.eq(df_1.iloc[i, 1]).any(axis=1)].iloc[0].name)\n",
    "        if (df_2.iloc[df_2_index,0] == df_1.iloc[i, 0]):\n",
    "            duplicate_count += 1\n",
    "\n",
    "print(duplicate_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    FIRST_TEXTS: List[str],\n",
    "    SECOND_TEXTS: List[str],\n",
    "    LABELS: List[int],\n",
    "    test_split: float = 0.1\n",
    "    ) -> [List[str], List[str], List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Splits a set of first and second texts and their labels into a training/test split\n",
    "    args:\n",
    "        FIRST_TEXTS,\n",
    "        SECOND_TEXTS,\n",
    "        LABELS,\n",
    "        test_split: float - the amount of test data points to extract from the existing set of data\n",
    "    returns:\n",
    "        2 sets of data\n",
    "    \"\"\"\n",
    "\n",
    "    # Init test lists and amended lists\n",
    "    TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS = [],[],[]\n",
    "    TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS = [],[],[]\n",
    "\n",
    "    TRAIN_FIRST_TEXTS, TEST_FIRST_TEXTS, TRAIN_SECOND_TEXTS, \\\n",
    "    TEST_SECOND_TEXTS, TRAIN_LABELS, TEST_LABELS = train_test_split(\n",
    "        FIRST_TEXTS, \n",
    "        SECOND_TEXTS, \n",
    "        LABELS, \n",
    "        test_size=test_split,\n",
    "        random_state=1234\n",
    "    )\n",
    "\n",
    "    return TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS, TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(csv_filepath: str, verbose: bool = False) -> [List[str], List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Prepares the data from csv files\n",
    "    args:\n",
    "        csv_filepath: str - the filepath of the csv file to load our authorship verification data from\n",
    "        verbose: bool\n",
    "    returns:\n",
    "        FIRST_TEXTS: List[str] - list of strings in the first column of csv \n",
    "        SECOND_TEXTS: List[str] - list of strings in the second column of csv \n",
    "        LABELS: List[int] - one hot encoded labels for whether the first and second text are from the same author\n",
    "    \"\"\"\n",
    "    FIRST_TEXTS, SECOND_TEXTS, LABELS = [],[],[]\n",
    "\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    # Iterate through and add to our first and second texts and labels\n",
    "    # I was initially worried about this being slow but it manages to do this on the train dataset very fast\n",
    "    for i in range(len(df)):\n",
    "        FIRST_TEXTS.append(df.iloc[i, 0])\n",
    "        SECOND_TEXTS.append(df.iloc[i, 1])\n",
    "        LABELS.append(df.iloc[i, 2])\n",
    "    \n",
    "    # Ensure that the data is valid\n",
    "    assert len(FIRST_TEXTS) == len(SECOND_TEXTS) == len(LABELS)\n",
    "    if verbose: print(\"Prepared\", len(df), \"data points.\")\n",
    "\n",
    "    return FIRST_TEXTS, SECOND_TEXTS, LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 30000 data points.\n",
      "Prepared 6000 data points.\n",
      "Training data size:  27000 \n",
      "Dev data size:  6000 \n",
      "Test data size:  3000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS  = prepareData(\"../data/train.csv\", True)\n",
    "DEV_FIRST_TEXTS, DEV_SECOND_TEXTS, DEV_LABELS = prepareData(\"../data/dev.csv\", True)\n",
    "\n",
    "TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS, \\\n",
    "TEST_FIRST_TEXTS, TEST_SECOND_TEXTS, TEST_LABELS = split_data(TRAIN_FIRST_TEXTS, TRAIN_SECOND_TEXTS, TRAIN_LABELS)\n",
    "\n",
    "print(\"Training data size: \", len(TRAIN_FIRST_TEXTS),\n",
    "     \"\\nDev data size: \", len(DEV_FIRST_TEXTS), \n",
    "     \"\\nTest data size: \", len(TEST_FIRST_TEXTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \"\"\"\n",
    "    For a given dataset, whether that's the training, dev, or \n",
    "    test dataset, this class handles most of it's setup and functions.\n",
    "\n",
    "    We will do all tokenization, feature extraction within this class and it will be what we pass to the model etc.\n",
    "\n",
    "    Variables:\n",
    "        FIRST_TEXTS - a list of the first texts,\n",
    "        SECOND_TEXTS - a list of the second texts,\n",
    "        LABELS - the binary classification (0 or 1) of whether this is from the same author,\n",
    "        FIRST_TOKENS - a list of the first texts, tokenized,\n",
    "        SECOND_TOKENS - a list of the first texts, tokenized,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, FIRST_TEXTS, SECOND_TEXTS, LABELS):\n",
    "        self.FIRST_TEXTS = FIRST_TEXTS\n",
    "        self.SECOND_TEXTS = SECOND_TEXTS\n",
    "        self.LABELS = LABELS\n",
    "    \n",
    "    def ExtractFeatures(self):\n",
    "\n",
    "\n",
    "\n",
    "## USE THIS: LightningDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   37, 32099, 10681,    16, 32098,  2447,     1]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "print(input_ids)\n",
    "print(type(input_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
